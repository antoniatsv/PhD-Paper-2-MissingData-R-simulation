"PSM")),
Validation_Dataset = str_remove(Validation_Dataset, "_val"),
Validation_Dataset = forcats::fct_recode(Validation_Dataset,
"PSM" = "raw",
"MI with Y" = "MIwithY",
"MI no Y" = "MInoY",
"Mean/Mode" = "mean_imputed"),
Validation_Dataset = forcats::fct_relevel(Validation_Dataset,
c("CCA",
"Mean/Mode",
"RI",
"MI with Y",
"MI no Y",
"PSM"))) %>%
rename("Target" = name) %>%
ggplot(aes(x = bias_mean,
y = Validation_Dataset,
color = Metric)) +
geom_point(aes(shape = Metric)) +
geom_errorbar(aes(xmin = bias_lower,
xmax = bias_upper), width=.1) +
scale_color_brewer(palette = "Set1") +
xlab("Bias") +
ylab("Validation Data Imputation Method") +
geom_vline(data = data.frame("Metric" = c("AUC",
"Brier Score",
"Calibration Intercept",
"Calibration Slope"),
"bias_mean" = c(0,0,0,0)),
aes(xintercept = bias_mean),
linetype = "dashed") +
ggh4x::facet_grid2(CPM ~ Target, scales = "fixed") +
ggtitle("Targetted Performance ") +
theme_minimal(base_size = 12) +
theme(legend.position = "bottom",
panel.background = element_rect(fill = "gray90"),
panel.spacing.x = unit(0.5, "lines"),
axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),
plot.title = element_text(size = 14, hjust = 0.5),
panel.border = element_rect(color = "black", fill = NA, linewidth = 0.5))
##plot results from the empirical study
Bootstrap_InternalValidation %>%
left_join(Bootstrap_InternalValidation %>%
filter(Validation_Dataset == "mean_imputed_val") %>%
select(-Validation_Dataset) %>%
rename("MeanMode_est" = "est",
"MeanMode_var" = "var"),
by = c("Bootstrap_Index", "CPM", "Metric")) %>%
left_join(Bootstrap_InternalValidation %>%
filter(Validation_Dataset == "RI_val") %>%
select(-Validation_Dataset) %>%
rename("RI_est" = "est",
"RI_var" = "var"),
by = c("Bootstrap_Index", "CPM", "Metric")) %>%
left_join(Bootstrap_InternalValidation %>%
filter(Validation_Dataset == "MInoY_val") %>%
select(-Validation_Dataset) %>%
rename("MInoY_est" = "est",
"MInoY_var" = "var"),
by = c("Bootstrap_Index", "CPM", "Metric")) %>%
left_join(Bootstrap_InternalValidation %>%
filter(Validation_Dataset == "raw_val")  %>%
select(-Validation_Dataset)%>%
rename("PSM_est" = "est",
"PSM_var" = "var"),
by = c("Bootstrap_Index", "CPM", "Metric")) %>%
select(Bootstrap_Index, CPM, Validation_Dataset, Metric, contains("est")) %>%
pivot_longer(cols = ends_with("_est")) %>%
mutate(bias = est - value) %>%
group_by(CPM, Validation_Dataset, Metric, name) %>%
summarise(bias_mean = mean(bias, na.rm = TRUE),
bias_lower = quantile(bias, 0.025, na.rm = TRUE),
bias_upper = quantile(bias, 0.975, na.rm = TRUE),
.groups = "drop") %>%
mutate(name = str_remove(name, "_est"),
name = forcats::fct_relevel(name,
c("MeanMode", "RI", "MInoY", "PSM")),
name = forcats::fct_recode(name,
"Targetting\n MI-no Y Performance" = "MInoY",
"Targetting\n Mean/Mode Performance" = "MeanMode",
"Targetting\n PSM Performance" = "PSM",
"Targetting\n RI Performance" = "RI"),
CPM = str_remove(CPM, "PR_"),
CPM = forcats::fct_recode(CPM,
"CCA\n developed CPM" = "CCA",
"RI\n developed CPM" = "RI",
"PSM\n developed CPM" = "patternsubmodel",
"MI with Y\n developed CPM" = "MIwithY",
"MI no Y\n developed CPM" = "MInoY",
"Mean/Mode\n developed CPM" = "meanimputed"),
CPM = forcats::fct_relevel(CPM,
c("CCA\n developed CPM",
"Mean/Mode\n developed CPM",
"RI\n developed CPM",
"MI with Y\n developed CPM",
"MI no Y\n developed CPM",
"PSM\n developed CPM")),
Validation_Dataset = str_remove(Validation_Dataset, "_val"),
Validation_Dataset = forcats::fct_recode(Validation_Dataset,
"PSM" = "raw",
"MI with Y" = "MIwithY",
"MI no Y" = "MInoY",
"Mean/Mode" = "mean_imputed"),
Validation_Dataset = forcats::fct_relevel(Validation_Dataset,
c("CCA",
"Mean/Mode",
"RI",
"MI with Y",
"MI no Y",
"PSM"))) %>%
rename("Target" = name) %>%
ggplot(aes(x = bias_mean,
y = Validation_Dataset,
color = Metric)) +
geom_point(aes(shape = Metric)) +
geom_errorbar(aes(xmin = bias_lower,
xmax = bias_upper), width=.1) +
scale_color_brewer(palette = "Set1") +
xlab("Bias") +
ylab("Validation Data Imputation Method") +
geom_vline(data = data.frame("Metric" = c("AUC",
"Brier Score",
"Calibration Intercept",
"Calibration Slope"),
"bias_mean" = c(0,0,0,0)),
aes(xintercept = bias_mean),
linetype = "dashed") +
ggh4x::facet_grid2(CPM ~ Target, scales = "fixed") +
ggtitle("Targetted Performance ") +
theme_minimal(base_size = 12) +
theme(legend.position = "bottom",
panel.background = element_rect(fill = "gray90"),
panel.spacing.x = unit(0.5, "lines"),
axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),
plot.title = element_text(size = 14, hjust = 0.5),
panel.border = element_rect(color = "black", fill = NA, linewidth = 0.5))
##plot results from the empirical study
Bootstrap_InternalValidation %>%
left_join(Bootstrap_InternalValidation %>%
filter(Validation_Dataset == "mean_imputed_val") %>%
select(-Validation_Dataset) %>%
rename("MeanMode_est" = "est",
"MeanMode_var" = "var"),
by = c("Bootstrap_Index", "CPM", "Metric")) %>%
left_join(Bootstrap_InternalValidation %>%
filter(Validation_Dataset == "RI_val") %>%
select(-Validation_Dataset) %>%
rename("RI_est" = "est",
"RI_var" = "var"),
by = c("Bootstrap_Index", "CPM", "Metric")) %>%
left_join(Bootstrap_InternalValidation %>%
filter(Validation_Dataset == "MInoY_val") %>%
select(-Validation_Dataset) %>%
rename("MInoY_est" = "est",
"MInoY_var" = "var"),
by = c("Bootstrap_Index", "CPM", "Metric")) %>%
left_join(Bootstrap_InternalValidation %>%
filter(Validation_Dataset == "raw_val")  %>%
select(-Validation_Dataset)%>%
rename("PSM_est" = "est",
"PSM_var" = "var"),
by = c("Bootstrap_Index", "CPM", "Metric")) %>%
select(Bootstrap_Index, CPM, Validation_Dataset, Metric, contains("est")) %>%
pivot_longer(cols = ends_with("_est")) %>%
mutate(bias = est - value) %>%
group_by(CPM, Validation_Dataset, Metric, name) %>%
summarise(bias_mean = mean(bias, na.rm = TRUE),
bias_lower = quantile(bias, 0.025, na.rm = TRUE),
bias_upper = quantile(bias, 0.975, na.rm = TRUE),
.groups = "drop") %>%
mutate(name = str_remove(name, "_est"),
name = forcats::fct_relevel(name,
c("MeanMode", "RI", "MInoY", "PSM")),
name = forcats::fct_recode(name,
"Targetting\n MI-no Y Performance" = "MInoY",
"Targetting\n Mean/Mode Performance" = "MeanMode",
"Targetting\n PSM Performance" = "PSM",
"Targetting\n RI Performance" = "RI"),
CPM = str_remove(CPM, "PR_"),
CPM = forcats::fct_recode(CPM,
"CCA\n developed CPM" = "CCA",
"RI\n developed CPM" = "RI",
"PSM\n developed CPM" = "patternsubmodel",
"MI with Y\n developed CPM" = "MIwithY",
"MI no Y\n developed CPM" = "MInoY",
"Mean/Mode\n developed CPM" = "meanimputed"),
CPM = forcats::fct_relevel(CPM,
c("CCA\n developed CPM",
"Mean/Mode\n developed CPM",
"RI\n developed CPM",
"MI with Y\n developed CPM",
"MI no Y\n developed CPM",
"PSM\n developed CPM")),
Validation_Dataset = str_remove(Validation_Dataset, "_val"),
Validation_Dataset = forcats::fct_recode(Validation_Dataset,
"PSM" = "raw",
"MI with Y" = "MIwithY",
"MI no Y" = "MInoY",
"Mean/Mode" = "mean_imputed"),
Validation_Dataset = forcats::fct_relevel(Validation_Dataset,
c("CCA",
"Mean/Mode",
"RI",
"MI with Y",
"MI no Y",
"PSM"))) %>%
rename("Target" = name) %>%
ggplot(aes(x = bias_mean,
y = Validation_Dataset,
color = Metric)) +
geom_point(aes(shape = Metric)) +
geom_errorbar(aes(xmin = bias_lower,
xmax = bias_upper), width=.1) +
scale_color_brewer(palette = "Set1") +
xlab("Bias") +
ylab("Validation Data Imputation Method") +
geom_vline(data = data.frame("Metric" = c("AUC",
"Brier Score",
"Calibration Intercept",
"Calibration Slope"),
"bias_mean" = c(0,0,0,0)),
aes(xintercept = bias_mean),
linetype = "dashed") +
ggh4x::facet_grid2(CPM ~ Target, scales = "fixed") +
ggtitle("Targetted Performance ") +
theme_minimal(base_size = 12) +
theme(legend.position = "bottom",
panel.background = element_rect(fill = "gray90"),
panel.spacing.x = unit(0.5, "lines"),
axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),
plot.title = element_text(size = 14, hjust = 0.5),
panel.border = element_rect(color = "black", fill = NA, linewidth = 0.5))
ggsave(file = here::here("Manuscript", "Fig6.tiff"))
ggsave(file = here::here("Manuscript", "Fig6.tiff"), height = 10, width = 10, dpi = 300)
library(tidyverse)
library(ggh4x)
simulation_results_summarised <- read_rds(here::here("outputs",
"simulation_results_summarised.RDS"))
simulation_results_summarised <- simulation_results_summarised %>%
dplyr::mutate("Scenario" = paste(paste(Missing_Mech_Dev, "Dev", sep=" "),
paste(Missing_Mech_Val, "Val", sep=" "),
sep = " -\n "),
Scenario = forcats::fct_relevel(Scenario,
"MCAR Dev -\n MCAR Val",
"MAR Dev -\n MAR Val",
"MNAR_X Dev -\n MNAR_X Val",
"MNAR_Y Dev -\n MNAR_Y Val",
"MNAR_XY Dev -\n MNAR_XY Val",
"MCAR Dev -\n MAR Val",
"MCAR Dev -\n MNAR_X Val",
"MCAR Dev -\n MNAR_Y Val",
"MCAR Dev -\n MNAR_XY Val",
"MAR Dev -\n MNAR_X Val",
"MAR Dev -\n MNAR_Y Val",
"MAR Dev -\n MNAR_XY Val",
"MNAR_X Dev -\n MNAR_Y Val",
"MNAR_X Dev -\n MNAR_XY Val"),
Validation_Dataset = forcats::fct_recode(Validation_Dataset,
"CCA" = "CCA_val",
"MI no Y (refit)" = "MI_val_noY_newimputation",
"MI no Y (transported)" = "MI_val_noY_transportedimputation",
"MI with Y (refit)" = "MI_val_withY_newimputation",
"MI with Y (transported)" = "MI_val_withY_transportedimputation",
"RI (refit)" = "RI_val_newimputation",
"RI (transported)" = "RI_val_transportedimputation",
"All Data Required" = "fullyobserved_val",
"Mean Imputation" = "mean_val",
"Risk Absent Imputation" ="zero_val",
"Pattern Sub-Model" = "observed_val"))
####----------------------------------------------------------------------------
### Select which estimand we want to produce plots for:
target_performance <- "All Data Required"
### Function to help summarise the results for a given iteration number:
plotting_fnc <- function(df,
scenario_number,
target_performance_imputation_method = c("All Data Required",
"CCA",
"Mean Imputation",
"RI (refit)",
"RI (transported)",
"MI no Y (refit)",
"MI no Y (transported)",
"MI with Y (refit)",
"MI with Y (transported)",
"Pattern Sub-Model")) {
target_performance_imputation_method <- as.character(match.arg(target_performance_imputation_method))
df <- df %>%
dplyr::mutate("combinations" = case_when(
CPM == "PR_fullyobserved" |
CPM == "PR_CCA" |
CPM == "PR_mean" ~ Validation_Dataset %in% c("All Data Required",
"CCA",
"Mean Imputation",
"MI with Y (refit)",
"MI no Y (refit)",
"RI (refit)"),
CPM == "PR_RI" ~ Validation_Dataset %in% c("All Data Required",
"CCA",
"Mean Imputation",
"MI with Y (refit)",
"MI no Y (refit)",
"RI (transported)",
"RI (refit)"),
CPM == "PR_MIwithY" ~ Validation_Dataset %in% c("All Data Required",
"CCA",
"Mean Imputation",
"MI with Y (transported)",
"MI with Y (refit)",
"MI no Y (refit)",
"RI (refit)"),
CPM == "PR_MInoY" ~ Validation_Dataset %in% c("All Data Required",
"CCA",
"Mean Imputation",
"MI with Y (refit)",
"MI no Y (transported)",
"MI no Y (refit)",
"RI (refit)"),
CPM == "PR_patternsubmodel" ~ Validation_Dataset %in% c("All Data Required",
"CCA",
"Mean Imputation",
"MI with Y (refit)",
"MI no Y (refit)",
"RI (refit)",
"Pattern Sub-Model"),
.default = NA)) %>%
dplyr::filter(combinations == 1)
scenario_df <- df %>%
dplyr::filter(Simulation_Scenario %in% scenario_number) %>%
dplyr::select(Simulation_Scenario, Scenario,
CPM, Validation_Dataset,
contains("_Mean"),
contains("_quantileLower"),
contains("_quantileUpper")) %>%
pivot_longer(cols = c(contains("_Mean"),
contains("_quantileLower"),
contains("_quantileUpper"))) %>%
separate_wider_delim(name,
delim = "_",
names = c("Metric", "Summary_Type")) %>%
pivot_wider(id_cols = c("Simulation_Scenario", "Scenario",
"CPM", "Validation_Dataset", "Metric"),
names_from = "Summary_Type",
values_from = "value")
results_using_for_implementation <- df %>%
dplyr::filter(Simulation_Scenario %in% scenario_number) %>%
dplyr::filter(Validation_Dataset == target_performance_imputation_method) %>%
dplyr::select(Simulation_Scenario, Scenario,
CPM, Validation_Dataset,
contains("_Mean"),
contains("_quantileLower"),
contains("_quantileUpper")) %>%
pivot_longer(cols = c(contains("_Mean"),
contains("_quantileLower"),
contains("_quantileUpper"))) %>%
separate_wider_delim(name,
delim = "_",
names = c("Metric", "Summary_Type")) %>%
pivot_wider(id_cols = c("Simulation_Scenario", "Scenario",
"CPM", "Validation_Dataset", "Metric"),
names_from = "Summary_Type",
values_from = "value") %>%
dplyr::rename("Imp_Data" = Validation_Dataset,
"Imp_Mean" = Mean,
"Imp_quantileLower" = quantileLower,
"Imp_quantileUpper" = quantileUpper)
plot_df <- scenario_df %>%
dplyr::left_join(results_using_for_implementation,
by = c("Simulation_Scenario",
"Scenario",
"CPM",
"Metric")) %>%
dplyr::mutate("Bias" = Mean - Imp_Mean,
"Bias_Lower" = quantileLower - Imp_quantileLower,
"Bias_Upper" = quantileUpper - Imp_quantileUpper) %>%
dplyr::mutate(Metric = forcats::fct_recode(Metric,
"Brier Score" = "Brier",
"Calibration Intercept" = "CalInt",
"Calibration Slope" = "CalSlope")) %>%
mutate(CPM = stringr::str_remove(CPM, "PR_"),
CPM = forcats::fct_recode(CPM,
"CCA \n developed CPM" = "CCA",
"RI \n developed CPM" = "RI",
"MI no Y \n developed CPM" = "MInoY",
"MI with Y \n developed CPM" = "MIwithY",
"Fully Observed \n developed CPM" = "fullyobserved",
"Mean Imputation \n developed CPM" = "mean",
"PSM \n developed CPM" = "patternsubmodel"),
CPM = forcats::fct_relevel(CPM,
"Fully Observed \n developed CPM",
"CCA \n developed CPM",
"Mean Imputation \n developed CPM",
"RI \n developed CPM",
"MI with Y \n developed CPM",
"MI no Y \n developed CPM",
"PSM \n developed CPM"))
plot_df %>%
tidyr::drop_na() %>%
dplyr::mutate(Validation_Dataset = forcats::fct_recode(Validation_Dataset,
"Fully Observed Data" = "All Data Required"),
Validation_Dataset = forcats::fct_relevel(Validation_Dataset,
"Fully Observed Data",
"CCA",
"Mean Imputation",
"RI (refit)",
"RI (transported)",
"MI with Y (refit)",
"MI with Y (transported)",
"MI no Y (refit)",
"MI no Y (transported)",
"Pattern Sub-Model")) %>%
ggplot(aes(x = Bias,
y = Validation_Dataset,
color = Metric)) +
geom_point(aes(shape = Metric)) +
geom_errorbar(aes(xmin = Bias_Lower,
xmax = Bias_Upper), width=.1) +
scale_color_brewer(palette = "Set1") +
xlab("Bias") +
ylab("Validation Data Imputation Method") +
geom_vline(data = data.frame("Metric" = c("AUC",
"Brier Score",
"Calibration Intercept",
"Calibration Slope"),
"Bias" = c(0,0,0,0)),
aes(xintercept = Bias),
linetype = "dashed") +
ggh4x::facet_grid2(Scenario ~ CPM, scales = "fixed") +
ggtitle(paste("Targetting ", target_performance_imputation_method, " Performance", sep = "")) +
theme_minimal(base_size = 12) +
theme(legend.position = "bottom",
panel.background = element_rect(fill = "gray90"),
panel.spacing.x = unit(0.5, "lines"),
axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),
plot.title = element_text(size = 14, hjust = 0.5),
panel.border = element_rect(color = "black", fill = NA, linewidth = 0.5))
}
#rho = 0.75
#MCAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(24, 56, 32, 64),
target_performance_imputation_method = target_performance)
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(184, 161, 193),
target_performance_imputation_method = target_performance)
#MNAR-X+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(416, 448, 256),
target_performance_imputation_method = target_performance)
## Inconsistent missingness mechanism plots:
#rho = 0
#MCAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(20, 52, 28, 60),
target_performance_imputation_method = target_performance)
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(180, 156, 188),
target_performance_imputation_method = target_performance)
#MNAR-X+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(416, 448, 256),
target_performance_imputation_method = target_performance)
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(184, 161, 193),
target_performance_imputation_method = target_performance)
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(180, 156, 188),
target_performance_imputation_method = target_performance)
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(184, 161, 193),
target_performance_imputation_method = target_performance)
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(185, 161, 193),
target_performance_imputation_method = target_performance)
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(185, 161, 192),
target_performance_imputation_method = target_performance)
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(184, 161, 192),
target_performance_imputation_method = target_performance)
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(180, 156, 188),
target_performance_imputation_method = target_performance)
## Inconsistent missingness mechanism plots:
#rho = 0
#MCAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(20, 52, 28, 60),
target_performance_imputation_method = target_performance)
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(180, 156, 188),
target_performance_imputation_method = target_performance)
## Inconsistent missingness mechanism plots:
#rho = 0
#MCAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(20, 52, 28, 60),
target_performance_imputation_method = target_performance)
target_performance <- "Mean Imputation"
#MAR+something
plotting_fnc(df = simulation_results_summarised,
scenario_number = c(180, 156, 188),
target_performance_imputation_method = target_performance)
